{
    "baseline": "The simplest algorithm doesn't need references :)",
    "catboost": "<li>Anna Veronika Dorogush, Andrey Gulin, Gleb Gusev, Nikita Kazeev, Liudmila Ostroumova Prokhorenkova, Aleksandr Vorobev, <a href='https://arxiv.org/abs/1706.09516' target='_blank' style='text-decoration:none;'>Fighting biases with dynamic boosting</a> , arXiv:1706.09516, 2017.</li><li>Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin, <a href='http://learningsys.org/nips17/assets/papers/paper_11.pdf' target='_blank' style='text-decoration:none;'>CatBoost: gradient boosting with categorical features support</a>, Workshop on ML Systems at NIPS 2017.</li>",
    "decision-tree": "<li>L. Breiman, J. Friedman, R. Olshen, and C. Stone, Classification and Regression Trees, 1984</li><li>T. Hastie, R. Tibshirani and J. Friedman., Elements of Statistical Learning, Springer, 2009.</li>",
    "extra-trees": "<li>P. Geurts, D. Ernst., and L. Wehenkel, <a href='https://link.springer.com/content/pdf/10.1007/s10994-006-6226-1.pdf' target='_blank' style='text-decoration:none;'>Extremely randomized trees</a>, Machine Learning, vol.63, pp.3-42, 2006</li>",
    "lightgbm": "<li>Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu, <a href='https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html' target='_blank' style='text-decoration:none;'>LightGBM: A Highly Efficient Gradient Boosting Decision Tree</a>, NIPS 2017, pp. 3149-3157.</li><li>Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu, <a href='https://proceedings.neurips.cc/paper/2016/hash/621bf66ddb7c962aa0d22ac97d69b793-Abstract.html' target='_blank' style='text-decoration:none;'>A Communication-Efficient Parallel Algorithm for Decision Tree</a>, NIPS 2016, pp. 1279-1287.</li>",
    "nearest-neighbors": "<li>Beyer, Kevin; et al. <a href='https://minds.wisconsin.edu/bitstream/handle/1793/60174/TR1377.pdf?sequence=1' target='_blank' style='text-decoration:none;'>'When is nearest neighbor meaningful?'</a>. Database Theoryâ€”ICDT'99. 1999: 217-235.</li>",
    "neural-network": "<li>Hinton, Geoffrey E., Connectionist learning procedures, Artificial intelligence, vol.40, pp.185-234, 1989</li>",
    "random-forest": "<li>Breiman Leo, <a href='https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf' target='_blank' style='text-decoration:none;'>Random Forests</a>, Machine Learning vol.45, pp. 5-32, 2001</li><li>Ho, Tin Kam, <a href='http://ect.bell-labs.com/who/tkh/publications/papers/odt.pdf' target='_blank' style='text-decoration:none;'>Random Decision Forests</a>, Proceedings of the 3rd International Conference on Document Analysis and Recognition, Montreal, pp. 278-282, 1995.</li>",
    "xgboost": "<li>Tianqi Chen and Carlos Guestrin, <a href='https://arxiv.org/abs/1603.02754' target='_blank' style='text-decoration:none;'>XGBoost: A Scalable Tree Boosting System</a>, In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016</li>"
}
