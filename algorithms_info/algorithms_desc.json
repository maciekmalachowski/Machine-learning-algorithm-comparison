{
    "baseline": "<b>Baseline</b> is the simplest algorithm that provides predictions without complex computations. For classification tasks, the Baseline returns the most frequent class. For regression tasks, the Baseline returns the average of the target from training data.",
    "catboost": "<a href='https://mljar.com/glossary/catboost/' target='_blank' style='text-decoration:none;'><b>CatBoost</b></a> provides Machine Learning algorithms under gradient boost framework developed by Yandex. It supports both numerical and categorical features.<br><br>It works on Linux, Windows, and macOS systems. It provides interfaces to Python and R. Trained model can be also used in C++, Java, C+, Rust, CoreML, ONNX, PMML.",
    "decision-tree": "<a href='https://mljar.com/glossary/decision-tree/' target='_blank' style='text-decoration:none;'><b>Decision Tree</b></a> is a supervised algorithm used in machine learning. It is using a binary tree graph (each node has two children) to assign for each data sample a target value. The target values are presented in the tree leaves. To reach the leaf, the sample is propagated through nodes, starting at the root node. In each node a decision is made, to which descendant node it should go. A decision is made based on the selected sampleâ€™s feature. It is usually one feature used to make the decision (one feature is used in the node to make a decision). Decision tree learning is a process of finding the optimal rules in each internal tree node according to the selected metric.",
    "extra-trees": "<a href='https://pro.arcgis.com/en/pro-app/latest/tool-reference/geoai/how-extra-tree-classification-and-regression-works.htm' target='_blank' style='text-decoration:none;'><b>Extra Trees</b></a> (Extremely Randomized Trees) the ensemble learning algorithms. It constructs the set of decision trees. During tree construction the decision rule is randomly selected. This algorithm is very similar to Random Forest except random selection of split values.",
    "lightgbm": "<a href='https://mljar.com/glossary/lightgbm/' target='_blank' style='text-decoration:none;'><b>LightGBM</b></a> (Light Gradient Boosting Machine) is a Machine Learning library that provides algorithms under gradient boosting framework developed by Microsoft.<br><br>It works on Linux, Windows, macOS, and supports C++, Python, R and C#.",
    "nearest-neighbors": "<a href='https://scikit-learn.org/stable/modules/neighbors.html' target='_blank' style='text-decoration:none;'><b>Nearest Neighbors</b></a> is a machine learning algorithm that classifies data points based on their proximity to other points. It uses a distance metric, like Euclidean distance, to find the <b>k</b> closest neighbors to a new data point. In classification, the most common label among these neighbors is assigned to the point. In regression, the average value is used. The algorithm is non-parametric and versatile, but it can be computationally expensive as it relies on the entire dataset during prediction. The choice of <b>k</b> is crucial for model performance.",
    "neural-network": "<a href='https://www.ibm.com/topics/neural-networks' target='_blank' style='text-decoration:none;'><b>Neural Network</b></a> (Multi-Layer Perceptron, MLP) is an algorithm inspired by biological neural networks. The MLP consists of connected graph of processing units that mimic the neurons. The connections between neurons are so-called weights. Their values are selected during the training process. The training goal is to minimize the error between values predicted by MLP and true values.",
    "random-forest": "<a href='https://mljar.com/glossary/random-forest/' target='_blank' style='text-decoration:none;'><b>Random Forest</b></a> is an ensemble learning algorithms that constructs many decision trees during the training. It predicts the mode of the classes for classification tasks and mean prediction of trees for regression tasks. It is using random subspace method and bagging during tree construction. It has built-in feature importance.",
    "xgboost": "<a href='https://mljar.com/glossary/xgboost/' target='_blank' style='text-decoration:none;'><b>XGBoost</b></a> (Extreme Gradient Boosting) is a library that provides machine learning algorithms under the a gradient boosting framework.<br><br>It works with major operating systems like Linux, Windows and macOS. It can run on a single machine or in the distributed environment with frameworks like Apache Hadoop, Apache Spark, Apache Flink, Dask, and DataFlow.<br><br>The library is available with interface in many programming languages: C++, Python, Java, R, Julia, Perl, and Scala."
}
