{
    "baseline":
    {
        "desc": "<b>Baseline</b> is the simplest algorithm that provides predictions without complex computations. For classification tasks, the Baseline returns the most frequent class. For regression tasks, the Baseline returns the average of the target from training data.",
        "license": "License for Scikit-Learn implementation of Baseline: New BSD License",
        "ref": "<li>Nathan de Lara, Edouard Pineau <a href='https://arxiv.org/abs/1810.09155' target='_blank' style='text-decoration:none;'>Baseline Algorithm for Graph Classification</a>,2018</li>"
    },

    "catboost":
    {
        "desc": "<a href='https://mljar.com/glossary/catboost/' target='_blank' style='text-decoration:none;'><b>CatBoost</b></a> provides Machine Learning algorithms under gradient boost framework developed by Yandex. It supports both numerical and categorical features.<br><br>It works on Linux, Windows, and macOS systems. It provides interfaces to Python and R. Trained model can be also used in C++, Java, C+, Rust, CoreML, ONNX, PMML.",
        "license": "Apache-2.0 License",
        "ref": "<li>Anna Veronika Dorogush, Andrey Gulin, Gleb Gusev, Nikita Kazeev, Liudmila Ostroumova Prokhorenkova, Aleksandr Vorobev, <a href='https://arxiv.org/abs/1706.09516' target='_blank' style='text-decoration:none;'>Fighting biases with dynamic boosting</a>, 2017.</li><li>Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin, <a href='http://learningsys.org/nips17/assets/papers/paper_11.pdf' target='_blank' style='text-decoration:none;'>CatBoost: gradient boosting with categorical features support</a>, Workshop on ML Systems at NIPS 2017.</li>"
    },

    "decision-tree":
    {
        "desc": "<a href='https://mljar.com/glossary/decision-tree/' target='_blank' style='text-decoration:none;'><b>Decision Tree</b></a> is a supervised algorithm used in machine learning. It is using a binary tree graph (each node has two children) to assign for each data sample a target value. The target values are presented in the tree leaves. To reach the leaf, the sample is propagated through nodes, starting at the root node. In each node a decision is made, to which descendant node it should go. A decision is made based on the selected sample’s feature. It is usually one feature used to make the decision (one feature is used in the node to make a decision). Decision tree learning is a process of finding the optimal rules in each internal tree node according to the selected metric.",
        "license": "License for Scikit-Learn implementation of Decision Tree: New BSD License",
        "ref": "<li>L. Breiman, J. Friedman, R. Olshen, and C. Stone, <a href='https://rafalab.dfci.harvard.edu/pages/649/section-11.pdf' target='_blank' style='text-decoration:none;'>Classification and Regression Trees</a>, 1984</li><li>T. Hastie, R. Tibshirani and J. Friedman., <a href='https://hastie.su.domains/Papers/ESLII.pdf' target='_blank' style='text-decoration:none;'>Elements of Statistical Learning</a>, Springer, 2009.</li>"
    },

    "extra-trees":
    {
        "desc": "<a href='https://pro.arcgis.com/en/pro-app/latest/tool-reference/geoai/how-extra-tree-classification-and-regression-works.htm' target='_blank' style='text-decoration:none;'><b>Extra Trees</b></a> (Extremely Randomized Trees) the ensemble learning algorithms. It constructs the set of decision trees. During tree construction the decision rule is randomly selected. This algorithm is very similar to Random Forest except random selection of split values.",
        "license": "License for Scikit-Learn implementation of Extra Trees: New BSD License",
        "ref": "<li>P. Geurts, D. Ernst., and L. Wehenkel, <a href='https://link.springer.com/content/pdf/10.1007/s10994-006-6226-1.pdf' target='_blank' style='text-decoration:none;'>Extremely randomized trees</a>, Machine Learning, vol.63, pp.3-42, 2006</li>"
    },

    "lightgbm":
    {
        "desc": "<a href='https://mljar.com/glossary/lightgbm/' target='_blank' style='text-decoration:none;'><b>LightGBM</b></a> (Light Gradient Boosting Machine) is a Machine Learning library that provides algorithms under gradient boosting framework developed by Microsoft.<br><br>It works on Linux, Windows, macOS, and supports C++, Python, R and C#.",
        "license": "MIT License",
        "ref": "<li>Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu, <a href='https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html' target='_blank' style='text-decoration:none;'>LightGBM: A Highly Efficient Gradient Boosting Decision Tree</a>, NIPS 2017, pp. 3149-3157.</li><li>Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu, <a href='https://proceedings.neurips.cc/paper/2016/hash/621bf66ddb7c962aa0d22ac97d69b793-Abstract.html' target='_blank' style='text-decoration:none;'>A Communication-Efficient Parallel Algorithm for Decision Tree</a>, NIPS 2016, pp. 1279-1287.</li>"
    },

    "nearest-neighbors":
    {
        "desc": "<a href='https://scikit-learn.org/stable/modules/neighbors.html' target='_blank' style='text-decoration:none;'><b>Nearest Neighbors</b></a> is a machine learning algorithm that classifies data points based on their proximity to other points. It uses a distance metric, like Euclidean distance, to find the <b>k</b> closest neighbors to a new data point. In classification, the most common label among these neighbors is assigned to the point. In regression, the average value is used. The algorithm is non-parametric and versatile, but it can be computationally expensive as it relies on the entire dataset during prediction. The choice of <b>k</b> is crucial for model performance.",
        "license": "License for Scikit-Learn implementation of Nearest Neighbors: New BSD License",
        "ref": "<li>Beyer, Kevin; et al. <a href='https://minds.wisconsin.edu/bitstream/handle/1793/60174/TR1377.pdf?sequence=1' target='_blank' style='text-decoration:none;'>'When is nearest neighbor meaningful?'</a>. Database Theory—ICDT'99. 1999: 217-235.</li>"
    },

    "neural-network":
    {
        "desc": "<a href='https://www.ibm.com/topics/neural-networks' target='_blank' style='text-decoration:none;'><b>Neural Network</b></a> (Multi-Layer Perceptron, MLP) is an algorithm inspired by biological neural networks. The MLP consists of connected graph of processing units that mimic the neurons. The connections between neurons are so-called weights. Their values are selected during the training process. The training goal is to minimize the error between values predicted by MLP and true values.",
        "license": "License for Scikit-Learn implementation of Neural Network: New BSD License",
        "ref": "<li>Hinton, Geoffrey E., <a href='https://www.cs.toronto.edu/~hinton/absps/clp.pdf' target='_blank' style='text-decoration:none;'>Connectionist learning procedures</a>, Artificial intelligence, vol.40, pp.185-234, 1989</li>"
    },

    "random-forest":
    {
        "desc": "<a href='https://mljar.com/glossary/random-forest/' target='_blank' style='text-decoration:none;'><b>Random Forest</b></a> is an ensemble learning algorithms that constructs many decision trees during the training. It predicts the mode of the classes for classification tasks and mean prediction of trees for regression tasks. It is using random subspace method and bagging during tree construction. It has built-in feature importance.",
        "license": "License for Scikit-Learn implementation of Random Forest: New BSD License",
        "ref": "<li>Breiman Leo, <a href='https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf' target='_blank' style='text-decoration:none;'>Random Forests</a>, Machine Learning vol.45, pp. 5-32, 2001</li><li>Ho, Tin Kam, <a href='http://ect.bell-labs.com/who/tkh/publications/papers/odt.pdf' target='_blank' style='text-decoration:none;'>Random Decision Forests</a>, Proceedings of the 3rd International Conference on Document Analysis and Recognition, Montreal, pp. 278-282, 1995.</li>"
    },

    "xgboost":
    {
        "desc": "<a href='https://mljar.com/glossary/xgboost/' target='_blank' style='text-decoration:none;'><b>XGBoost</b></a> (Extreme Gradient Boosting) is a library that provides machine learning algorithms under the a gradient boosting framework.<br><br>It works with major operating systems like Linux, Windows and macOS. It can run on a single machine or in the distributed environment with frameworks like Apache Hadoop, Apache Spark, Apache Flink, Dask, and DataFlow.<br><br>The library is available with interface in many programming languages: C++, Python, Java, R, Julia, Perl, and Scala.",
        "license": "Apache-2.0 License",
        "ref": "<li>Tianqi Chen and Carlos Guestrin, <a href='https://arxiv.org/abs/1603.02754' target='_blank' style='text-decoration:none;'>XGBoost: A Scalable Tree Boosting System</a>, In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016</li>"
    }
}